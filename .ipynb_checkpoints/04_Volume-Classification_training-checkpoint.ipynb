{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jain0000/miniconda3/envs/segment2/lib/python3.7/site-packages/dicom/__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from niwidgets import NiftiWidget\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import * \n",
    "from tensorflow import keras\n",
    "from config import *\n",
    "from modelIO import *\n",
    "from model import *\n",
    "from display import *\n",
    "from dataAugmentation import *\n",
    "from classification import *\n",
    "import math\n",
    "import random\n",
    "from skimage.util import montage, crop\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.metrics import MeanIoU,BinaryCrossentropy,Accuracy,Precision,Recall\n",
    "from skimage.transform import rescale, resize, downscale_local_mean,rotate\n",
    "from skimage import data, color\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from numpy import fliplr\n",
    "from PIL import Image\n",
    "from datetime import *\n",
    "from sklearn.model_selection import KFold\n",
    "#print(device_lib.list_local_devices())\n",
    "#tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load('segmentation')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mandible-Right-Molar/iteration1\n",
      "mandible-Right-Molar/iteration1\n",
      "mandible-Right-Molar/iteration1\n",
      "mandible-Right-Molar/iteration1\n",
      "mandible-Right-Molar/iteration1\n",
      "mandible-Right-Canine/iteration3\n",
      "mandible-Right-Canine/iteration3\n",
      "mandible-Right-Canine/iteration3\n",
      "mandible-Right-Canine/iteration3\n",
      "mandible-Right-Canine/iteration3\n",
      "maxilla-Right-Molar/iteration1\n",
      "maxilla-Right-Molar/iteration1\n",
      "maxilla-Right-Molar/iteration1\n",
      "maxilla-Right-Molar/iteration1\n",
      "maxilla-Right-Molar/iteration1\n",
      "(3, 5, 40, 40, 40)\n",
      "(3, 5)\n",
      "mandible-Right-Canine\n",
      "mandible-Right-Molar\n",
      "maxilla-Left-Canine\n",
      "mandible-Right-Premolar\n",
      "maxilla-Right-Canine\n",
      "mandible-Left-Molar\n",
      "maxilla-Left-Premolar\n",
      "maxilla-Left-Incisor\n",
      "mandible-Left-Incisor\n",
      "mandible-Right-Incisor\n",
      "maxilla-Right-Premolar\n",
      "maxilla-Left-Molar\n",
      "maxilla-Right-Incisor\n",
      "mandible-Left-Premolar\n",
      "maxilla-Right-Molar\n",
      "mandible-Left-Canine\n"
     ]
    }
   ],
   "source": [
    "input_arr1=[]\n",
    "input_arr2_mask=[]\n",
    "input_arr3=[]\n",
    "input_arr4=[]\n",
    "input_arr2_class=[]\n",
    "for y in ['mandible-Right-Molar/iteration1','mandible-Right-Canine/iteration3','maxilla-Right-Molar/iteration1']:\n",
    "    for t in os.listdir(os.path.join(data_dir_train+str('image'))):\n",
    "        if y.find(t) != -1:\n",
    "            d = dataset(y,'train')\n",
    "            x_train=d[0]\n",
    "            y_train_class=d[2]\n",
    "            y_train_mask=d[1]\n",
    "            input_arr1.append(x_train)\n",
    "            input_arr2_mask.append(y_train_mask)\n",
    "            input_arr2_class.append(y_train_class)\n",
    "            \n",
    "print(np.array(input_arr2_mask).shape)\n",
    "print(np.array(input_arr2_class).shape)\n",
    "\n",
    "for t in os.listdir(os.path.join(data_dir_test+str('image'))):\n",
    "    d = dataset(t,'test')\n",
    "    #print(d[2])\n",
    "   # display_monatage(d[0][0],d[1][0])\n",
    "    x_train=d[0]\n",
    "    y_train=d[1]\n",
    "    input_arr3.append(x_train)\n",
    "    input_arr4.append(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 40, 40, 40) (15, 40, 40, 40) (15,)\n"
     ]
    }
   ],
   "source": [
    "fold_no = 1  \n",
    "score=[]\n",
    "models =[]\n",
    "val=[]\n",
    "\n",
    "name  = '-'.join(['UNET3D-Tooth','classification',str(IMAGE_HEIGHT),str(IMAGE_WIDTH),str(IMAGE_DEPTH),datetime.now().strftime(\"%Y%m%d-%H%M%S\"),'.h5'])   \n",
    "logdir = LOG_PATH + name\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "inputs_train = np.array(input_arr1)\n",
    "target_train = np.array(input_arr2_mask)\n",
    "target_train_class =np.array(input_arr2_class)\n",
    "inputs_val = np.array(input_arr3)\n",
    "target_val = np.array(input_arr4)\n",
    "\n",
    "inputsAll=np.concatenate((inputs_train),axis=0)\n",
    "target=np.concatenate((target_train),axis=0)\n",
    "classLabel =np.concatenate((target_train_class),axis=0)\n",
    "print(inputsAll.shape,target.shape,classLabel.shape)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_preprocessing at 0x7febe1981050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function train_preprocessing at 0x7febe1981050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _fixup_shape at 0x7febe1981170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _fixup_shape at 0x7febe1981170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "def train_preprocessing(volume, label,classLabel):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    #if DATA_AUG==True:\n",
    "   #     volume,label = data_aug(volume,label)\n",
    "    \n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    label = tf.expand_dims(tf.math.round(label), axis=3)\n",
    "    classLabel = tf.expand_dims(classLabel, axis=0)\n",
    "    return volume,label,classLabel\n",
    "\n",
    "\n",
    "def _fixup_shape(images, labels,classLabel):\n",
    "    images.set_shape([None,None,None,None])\n",
    "    labels.set_shape([None,None,None,None]) # I have 19 classes\n",
    "    classLabel.set_shape([None])\n",
    "    return images, labels,classLabel\n",
    "\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((inputsAll, target,classLabel))\n",
    "#validation_loader = tf.data.Dataset.from_tensor_slices((inputs_val,target_val,classLabel))\n",
    "    \n",
    "train_dataset = (\n",
    "train_loader\n",
    ".map(train_preprocessing)\n",
    ".map(_fixup_shape)\n",
    ".batch(30)  \n",
    ".prefetch(30)\n",
    ")\n",
    "# Only rescale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNET3D-L4-F32\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40, 40, 40,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d (Conv3D)                 (None, 40, 40, 40, 3 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 40, 40, 40, 3 128         conv3d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu (TensorFlowOpL [(None, 40, 40, 40,  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 40, 40, 40, 3 27680       tf_op_layer_Relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 40, 40, 40, 3 128         conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_1 (TensorFlowO [(None, 40, 40, 40,  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 40, 40, 40, 6 55360       tf_op_layer_Relu_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 40, 40, 40, 6 256         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_2 (TensorFlowO [(None, 40, 40, 40,  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 40, 40, 40, 6 110656      tf_op_layer_Relu_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 40, 40, 40, 6 256         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_3 (TensorFlowO [(None, 40, 40, 40,  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 40, 40, 40, 1 221312      tf_op_layer_Relu_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 40, 40, 40, 1 512         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_4 (TensorFlowO [(None, 40, 40, 40,  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 40, 40, 40, 1 442496      tf_op_layer_Relu_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 40, 40, 40, 1 512         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_5 (TensorFlowO [(None, 40, 40, 40,  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 20, 20, 20, 1 0           tf_op_layer_Relu_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 20, 20, 20, 1 1493008     max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 20, 20, 1 64          conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_14 (TensorFlow [(None, 20, 20, 20,  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTrans (None, 40, 40, 40, 3 110624      max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, 10, 10, 10, 1 0           tf_op_layer_Relu_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40, 40, 40, 6 0           conv3d_transpose_2[0][0]         \n",
      "                                                                 tf_op_layer_Relu_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling3d (Globa (None, 16)           0           max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 40, 40, 40, 3 55328       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16)           64          global_average_pooling3d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 40, 40, 40, 3 128         conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           272         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_12 (TensorFlow [(None, 40, 40, 40,  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 40, 40, 40, 3 27680       tf_op_layer_Relu_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 40, 40, 40, 3 128         conv3d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu_13 (TensorFlow [(None, 40, 40, 40,  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            36          batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 40, 40, 40, 3 0           tf_op_layer_Relu_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 40, 40, 40, 1 33          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            25          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,547,814\n",
      "Trainable params: 2,546,678\n",
      "Non-trainable params: 1,136\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#print(np.expand_dims(np.array(np.concatenate((im),axis=0)),4).shape)\n",
    "\n",
    "model= get_model('segmentation_and_classification')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feacc0be250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7feacc0be250>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch: 0\n",
      "Loss: 1.7989897727966309\n",
      "Epoch: 1\n",
      "Loss: 1.786312460899353\n",
      "Epoch: 2\n",
      "Loss: 1.7586053609848022\n",
      "Epoch: 3\n",
      "Loss: 1.686808705329895\n",
      "Epoch: 4\n",
      "Loss: 1.6639404296875\n",
      "Epoch: 5\n",
      "Loss: 1.6629334688186646\n",
      "Epoch: 6\n",
      "Loss: 1.662138819694519\n",
      "Epoch: 7\n",
      "Loss: 1.661345362663269\n",
      "Epoch: 8\n",
      "Loss: 1.660553216934204\n",
      "Epoch: 9\n",
      "Loss: 1.6597617864608765\n",
      "Epoch: 10\n",
      "Loss: 1.6589717864990234\n",
      "Epoch: 11\n",
      "Loss: 1.6581827402114868\n",
      "Epoch: 12\n",
      "Loss: 1.6573951244354248\n",
      "Epoch: 13\n",
      "Loss: 1.6566084623336792\n",
      "Epoch: 14\n",
      "Loss: 1.65582275390625\n",
      "Epoch: 15\n",
      "Loss: 1.6550384759902954\n",
      "Epoch: 16\n",
      "Loss: 1.6542553901672363\n",
      "Epoch: 17\n",
      "Loss: 1.6534733772277832\n",
      "Epoch: 18\n",
      "Loss: 1.6526925563812256\n",
      "Epoch: 19\n",
      "Loss: 1.6519131660461426\n",
      "Epoch: 20\n",
      "Loss: 1.651134729385376\n",
      "Epoch: 21\n",
      "Loss: 1.650357723236084\n",
      "Epoch: 22\n",
      "Loss: 1.6495819091796875\n",
      "Epoch: 23\n",
      "Loss: 1.6488072872161865\n",
      "Epoch: 24\n",
      "Loss: 1.6480340957641602\n",
      "Epoch: 25\n",
      "Loss: 1.6472619771957397\n",
      "Epoch: 26\n",
      "Loss: 1.646491289138794\n",
      "Epoch: 27\n",
      "Loss: 1.6457217931747437\n",
      "Epoch: 28\n",
      "Loss: 1.6449536085128784\n",
      "Epoch: 29\n",
      "Loss: 1.6441867351531982\n",
      "Epoch: 30\n",
      "Loss: 1.6434210538864136\n",
      "Epoch: 31\n",
      "Loss: 1.642656922340393\n",
      "Epoch: 32\n",
      "Loss: 1.641893982887268\n",
      "Epoch: 33\n",
      "Loss: 1.6411322355270386\n",
      "Epoch: 34\n",
      "Loss: 1.6403719186782837\n",
      "Epoch: 35\n",
      "Loss: 1.6396127939224243\n",
      "Epoch: 36\n",
      "Loss: 1.638855218887329\n",
      "Epoch: 37\n",
      "Loss: 1.6380988359451294\n",
      "Epoch: 38\n",
      "Loss: 1.6373437643051147\n",
      "Epoch: 39\n",
      "Loss: 1.6365900039672852\n",
      "Epoch: 40\n",
      "Loss: 1.6358377933502197\n",
      "Epoch: 41\n",
      "Loss: 1.6350867748260498\n",
      "Epoch: 42\n",
      "Loss: 1.6343368291854858\n",
      "Epoch: 43\n",
      "Loss: 1.633588433265686\n",
      "Epoch: 44\n",
      "Loss: 1.6328414678573608\n",
      "Epoch: 45\n",
      "Loss: 1.6320958137512207\n",
      "Epoch: 46\n",
      "Loss: 1.631351351737976\n",
      "Epoch: 47\n",
      "Loss: 1.6306084394454956\n",
      "Epoch: 48\n",
      "Loss: 1.6298667192459106\n",
      "Epoch: 49\n",
      "Loss: 1.6291264295578003\n",
      "Epoch: 50\n",
      "Loss: 1.6283873319625854\n",
      "Epoch: 51\n",
      "Loss: 1.6276497840881348\n",
      "Epoch: 52\n",
      "Loss: 1.6269134283065796\n",
      "Epoch: 53\n",
      "Loss: 1.62617826461792\n",
      "Epoch: 54\n",
      "Loss: 1.625444769859314\n",
      "Epoch: 55\n",
      "Loss: 1.624712347984314\n",
      "Epoch: 56\n",
      "Loss: 1.623981237411499\n",
      "Epoch: 57\n",
      "Loss: 1.6232517957687378\n",
      "Epoch: 58\n",
      "Loss: 1.622523307800293\n",
      "Epoch: 59\n",
      "Loss: 1.6217962503433228\n",
      "Epoch: 60\n",
      "Loss: 1.6210706233978271\n",
      "Epoch: 61\n",
      "Loss: 1.6203463077545166\n",
      "Epoch: 62\n",
      "Loss: 1.6196231842041016\n",
      "Epoch: 63\n",
      "Loss: 1.6189014911651611\n",
      "Epoch: 64\n",
      "Loss: 1.6181812286376953\n",
      "Epoch: 65\n",
      "Loss: 1.6174620389938354\n",
      "Epoch: 66\n",
      "Loss: 1.6167442798614502\n",
      "Epoch: 67\n",
      "Loss: 1.61602783203125\n",
      "Epoch: 68\n",
      "Loss: 1.6153128147125244\n",
      "Epoch: 69\n",
      "Loss: 1.6145989894866943\n",
      "Epoch: 70\n",
      "Loss: 1.6138864755630493\n",
      "Epoch: 71\n",
      "Loss: 1.6131752729415894\n",
      "Epoch: 72\n",
      "Loss: 1.6124653816223145\n",
      "Epoch: 73\n",
      "Loss: 1.6117568016052246\n",
      "Epoch: 74\n",
      "Loss: 1.6110495328903198\n",
      "Epoch: 75\n",
      "Loss: 1.6103435754776\n",
      "Epoch: 76\n",
      "Loss: 1.6096389293670654\n",
      "Epoch: 77\n",
      "Loss: 1.6089354753494263\n",
      "Epoch: 78\n",
      "Loss: 1.6082333326339722\n",
      "Epoch: 79\n",
      "Loss: 1.6075327396392822\n",
      "Epoch: 80\n",
      "Loss: 1.6068331003189087\n",
      "Epoch: 81\n",
      "Loss: 1.6061348915100098\n",
      "Epoch: 82\n",
      "Loss: 1.6054378747940063\n",
      "Epoch: 83\n",
      "Loss: 1.604742407798767\n",
      "Epoch: 84\n",
      "Loss: 1.6040481328964233\n",
      "Epoch: 85\n",
      "Loss: 1.603354811668396\n",
      "Epoch: 86\n",
      "Loss: 1.6026630401611328\n",
      "Epoch: 87\n",
      "Loss: 1.6019724607467651\n",
      "Epoch: 88\n",
      "Loss: 1.601283311843872\n",
      "Epoch: 89\n",
      "Loss: 1.6005953550338745\n",
      "Epoch: 90\n",
      "Loss: 1.5999085903167725\n",
      "Epoch: 91\n",
      "Loss: 1.599223256111145\n",
      "Epoch: 92\n",
      "Loss: 1.5985389947891235\n",
      "Epoch: 93\n",
      "Loss: 1.597856044769287\n",
      "Epoch: 94\n",
      "Loss: 1.5971745252609253\n",
      "Epoch: 95\n",
      "Loss: 1.5964939594268799\n",
      "Epoch: 96\n",
      "Loss: 1.595814824104309\n",
      "Epoch: 97\n",
      "Loss: 1.5951370000839233\n",
      "Epoch: 98\n",
      "Loss: 1.594460368156433\n",
      "Epoch: 99\n",
      "Loss: 1.5937849283218384\n",
      "Epoch: 100\n",
      "Loss: 1.5931107997894287\n",
      "Epoch: 101\n",
      "Loss: 1.592437982559204\n",
      "Epoch: 102\n",
      "Loss: 1.591766119003296\n",
      "Epoch: 103\n",
      "Loss: 1.5910958051681519\n",
      "Epoch: 104\n",
      "Loss: 1.5904265642166138\n",
      "Epoch: 105\n",
      "Loss: 1.5897587537765503\n",
      "Epoch: 106\n",
      "Loss: 1.5890920162200928\n",
      "Epoch: 107\n",
      "Loss: 1.5884263515472412\n",
      "Epoch: 108\n",
      "Loss: 1.5877623558044434\n",
      "Epoch: 109\n",
      "Loss: 1.587099313735962\n",
      "Epoch: 110\n",
      "Loss: 1.5864375829696655\n",
      "Epoch: 111\n",
      "Loss: 1.585776925086975\n",
      "Epoch: 112\n",
      "Loss: 1.5851175785064697\n",
      "Epoch: 113\n",
      "Loss: 1.5844593048095703\n",
      "Epoch: 114\n",
      "Loss: 1.583802580833435\n",
      "Epoch: 115\n",
      "Loss: 1.5831468105316162\n",
      "Epoch: 116\n",
      "Loss: 1.5824922323226929\n",
      "Epoch: 117\n",
      "Loss: 1.5818392038345337\n",
      "Epoch: 118\n",
      "Loss: 1.581187129020691\n",
      "Epoch: 119\n",
      "Loss: 1.5805362462997437\n",
      "Epoch: 120\n",
      "Loss: 1.579886555671692\n",
      "Epoch: 121\n",
      "Loss: 1.5792380571365356\n",
      "Epoch: 122\n",
      "Loss: 1.5785908699035645\n",
      "Epoch: 123\n",
      "Loss: 1.5779446363449097\n",
      "Epoch: 124\n",
      "Loss: 1.577299952507019\n",
      "Epoch: 125\n",
      "Loss: 1.5766562223434448\n",
      "Epoch: 126\n",
      "Loss: 1.5760135650634766\n",
      "Epoch: 127\n",
      "Loss: 1.5753724575042725\n",
      "Epoch: 128\n",
      "Loss: 1.5747324228286743\n",
      "Epoch: 129\n",
      "Loss: 1.5740935802459717\n",
      "Epoch: 130\n",
      "Loss: 1.5734556913375854\n",
      "Epoch: 131\n",
      "Loss: 1.5728192329406738\n",
      "Epoch: 132\n",
      "Loss: 1.572183609008789\n",
      "Epoch: 133\n",
      "Loss: 1.571549654006958\n",
      "Epoch: 134\n",
      "Loss: 1.5709166526794434\n",
      "Epoch: 135\n",
      "Loss: 1.5702849626541138\n",
      "Epoch: 136\n",
      "Loss: 1.569654107093811\n",
      "Epoch: 137\n",
      "Loss: 1.5690245628356934\n",
      "Epoch: 138\n",
      "Loss: 1.5683962106704712\n",
      "Epoch: 139\n",
      "Loss: 1.567769169807434\n",
      "Epoch: 140\n",
      "Loss: 1.5671430826187134\n",
      "Epoch: 141\n",
      "Loss: 1.5665181875228882\n",
      "Epoch: 142\n",
      "Loss: 1.5658944845199585\n",
      "Epoch: 143\n",
      "Loss: 1.5652719736099243\n",
      "Epoch: 144\n",
      "Loss: 1.5646506547927856\n",
      "Epoch: 145\n",
      "Loss: 1.5640302896499634\n",
      "Epoch: 146\n",
      "Loss: 1.5634111166000366\n",
      "Epoch: 147\n",
      "Loss: 1.5627933740615845\n",
      "Epoch: 148\n",
      "Loss: 1.5621764659881592\n",
      "Epoch: 149\n",
      "Loss: 1.5615609884262085\n",
      "Epoch: 150\n",
      "Loss: 1.5609462261199951\n",
      "Epoch: 151\n",
      "Loss: 1.5603328943252563\n",
      "Epoch: 152\n",
      "Loss: 1.5597206354141235\n",
      "Epoch: 153\n",
      "Loss: 1.5591095685958862\n",
      "Epoch: 154\n",
      "Loss: 1.5584995746612549\n",
      "Epoch: 155\n",
      "Loss: 1.557890772819519\n",
      "Epoch: 156\n",
      "Loss: 1.5572830438613892\n",
      "Epoch: 157\n",
      "Loss: 1.5566762685775757\n",
      "Epoch: 158\n",
      "Loss: 1.5560709238052368\n",
      "Epoch: 159\n",
      "Loss: 1.5554665327072144\n",
      "Epoch: 160\n",
      "Loss: 1.554863452911377\n",
      "Epoch: 161\n",
      "Loss: 1.5542610883712769\n",
      "Epoch: 162\n",
      "Loss: 1.5536600351333618\n",
      "Epoch: 163\n",
      "Loss: 1.5530601739883423\n",
      "Epoch: 164\n",
      "Loss: 1.5524615049362183\n",
      "Epoch: 165\n",
      "Loss: 1.5518637895584106\n",
      "Epoch: 166\n",
      "Loss: 1.551267147064209\n",
      "Epoch: 167\n",
      "Loss: 1.5506718158721924\n",
      "Epoch: 168\n",
      "Loss: 1.5500774383544922\n",
      "Epoch: 169\n",
      "Loss: 1.549484133720398\n",
      "Epoch: 170\n",
      "Loss: 1.5488919019699097\n",
      "Epoch: 171\n",
      "Loss: 1.548301100730896\n",
      "Epoch: 172\n",
      "Loss: 1.5477110147476196\n",
      "Epoch: 173\n",
      "Loss: 1.5471221208572388\n",
      "Epoch: 174\n",
      "Loss: 1.5465344190597534\n",
      "Epoch: 175\n",
      "Loss: 1.5459476709365845\n",
      "Epoch: 176\n",
      "Loss: 1.545362114906311\n",
      "Epoch: 177\n",
      "Loss: 1.5447776317596436\n",
      "Epoch: 178\n",
      "Loss: 1.544193983078003\n",
      "Epoch: 179\n",
      "Loss: 1.543611764907837\n",
      "Epoch: 180\n",
      "Loss: 1.5430303812026978\n",
      "Epoch: 181\n",
      "Loss: 1.542450189590454\n",
      "Epoch: 182\n",
      "Loss: 1.5418710708618164\n",
      "Epoch: 183\n",
      "Loss: 1.5412930250167847\n",
      "Epoch: 184\n",
      "Loss: 1.5407160520553589\n",
      "Epoch: 185\n",
      "Loss: 1.5401400327682495\n",
      "Epoch: 186\n",
      "Loss: 1.5395652055740356\n",
      "Epoch: 187\n",
      "Loss: 1.5389913320541382\n",
      "Epoch: 188\n",
      "Loss: 1.5384185314178467\n",
      "Epoch: 189\n",
      "Loss: 1.5378469228744507\n",
      "Epoch: 190\n",
      "Loss: 1.5372763872146606\n",
      "Epoch: 191\n",
      "Loss: 1.5367066860198975\n",
      "Epoch: 192\n",
      "Loss: 1.5361382961273193\n",
      "Epoch: 193\n",
      "Loss: 1.5355709791183472\n",
      "Epoch: 194\n",
      "Loss: 1.5350044965744019\n",
      "Epoch: 195\n",
      "Loss: 1.5344390869140625\n",
      "Epoch: 196\n",
      "Loss: 1.5338749885559082\n",
      "Epoch: 197\n",
      "Loss: 1.5333114862442017\n",
      "Epoch: 198\n",
      "Loss: 1.5327494144439697\n",
      "Epoch: 199\n",
      "Loss: 1.532188057899475\n",
      "Epoch: 200\n",
      "Loss: 1.531628131866455\n",
      "Epoch: 201\n",
      "Loss: 1.5310688018798828\n",
      "Epoch: 202\n",
      "Loss: 1.5305109024047852\n",
      "Epoch: 203\n",
      "Loss: 1.5299537181854248\n",
      "Epoch: 204\n",
      "Loss: 1.5293976068496704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205\n",
      "Loss: 1.528842806816101\n",
      "Epoch: 206\n",
      "Loss: 1.5282888412475586\n",
      "Epoch: 207\n",
      "Loss: 1.5277358293533325\n",
      "Epoch: 208\n",
      "Loss: 1.527184009552002\n",
      "Epoch: 209\n",
      "Loss: 1.5266331434249878\n",
      "Epoch: 210\n",
      "Loss: 1.52608323097229\n",
      "Epoch: 211\n",
      "Loss: 1.5255343914031982\n",
      "Epoch: 212\n",
      "Loss: 1.5249865055084229\n",
      "Epoch: 213\n",
      "Loss: 1.5244396924972534\n",
      "Epoch: 214\n",
      "Loss: 1.52389395236969\n",
      "Epoch: 215\n",
      "Loss: 1.5233490467071533\n",
      "Epoch: 216\n",
      "Loss: 1.5228052139282227\n",
      "Epoch: 217\n",
      "Loss: 1.522262454032898\n",
      "Epoch: 218\n",
      "Loss: 1.5217206478118896\n",
      "Epoch: 219\n",
      "Loss: 1.5211799144744873\n",
      "Epoch: 220\n",
      "Loss: 1.5206401348114014\n",
      "Epoch: 221\n",
      "Loss: 1.5201011896133423\n",
      "Epoch: 222\n",
      "Loss: 1.5195634365081787\n",
      "Epoch: 223\n",
      "Loss: 1.519026756286621\n",
      "Epoch: 224\n",
      "Loss: 1.5184907913208008\n",
      "Epoch: 225\n",
      "Loss: 1.517956018447876\n",
      "Epoch: 226\n",
      "Loss: 1.5174221992492676\n",
      "Epoch: 227\n",
      "Loss: 1.5168893337249756\n",
      "Epoch: 228\n",
      "Loss: 1.5163573026657104\n",
      "Epoch: 229\n",
      "Loss: 1.5158265829086304\n",
      "Epoch: 230\n",
      "Loss: 1.5152968168258667\n",
      "Epoch: 231\n",
      "Loss: 1.5147677659988403\n",
      "Epoch: 232\n",
      "Loss: 1.5142399072647095\n",
      "Epoch: 233\n",
      "Loss: 1.5137128829956055\n",
      "Epoch: 234\n",
      "Loss: 1.5131868124008179\n",
      "Epoch: 235\n",
      "Loss: 1.5126619338989258\n",
      "Epoch: 236\n",
      "Loss: 1.512137770652771\n",
      "Epoch: 237\n",
      "Loss: 1.5116147994995117\n",
      "Epoch: 238\n",
      "Loss: 1.5110925436019897\n",
      "Epoch: 239\n",
      "Loss: 1.5105714797973633\n",
      "Epoch: 240\n",
      "Loss: 1.5100511312484741\n",
      "Epoch: 241\n",
      "Loss: 1.509531855583191\n",
      "Epoch: 242\n",
      "Loss: 1.5090137720108032\n",
      "Epoch: 243\n",
      "Loss: 1.5084962844848633\n",
      "Epoch: 244\n",
      "Loss: 1.5079799890518188\n",
      "Epoch: 245\n",
      "Loss: 1.5074645280838013\n",
      "Epoch: 246\n",
      "Loss: 1.5069500207901\n",
      "Epoch: 247\n",
      "Loss: 1.5064363479614258\n",
      "Epoch: 248\n",
      "Loss: 1.505923867225647\n",
      "Epoch: 249\n",
      "Loss: 1.505412220954895\n",
      "Epoch: 250\n",
      "Loss: 1.5049015283584595\n",
      "Epoch: 251\n",
      "Loss: 1.5043916702270508\n",
      "Epoch: 252\n",
      "Loss: 1.5038830041885376\n",
      "Epoch: 253\n",
      "Loss: 1.5033751726150513\n",
      "Epoch: 254\n",
      "Loss: 1.5028680562973022\n",
      "Epoch: 255\n",
      "Loss: 1.5023620128631592\n",
      "Epoch: 256\n",
      "Loss: 1.5018571615219116\n",
      "Epoch: 257\n",
      "Loss: 1.5013530254364014\n",
      "Epoch: 258\n",
      "Loss: 1.5008498430252075\n",
      "Epoch: 259\n",
      "Loss: 1.500347375869751\n",
      "Epoch: 260\n",
      "Loss: 1.49984610080719\n",
      "Epoch: 261\n",
      "Loss: 1.4993455410003662\n",
      "Epoch: 262\n",
      "Loss: 1.4988460540771484\n",
      "Epoch: 263\n",
      "Loss: 1.4983476400375366\n",
      "Epoch: 264\n",
      "Loss: 1.497849702835083\n",
      "Epoch: 265\n",
      "Loss: 1.4973530769348145\n",
      "Epoch: 266\n",
      "Loss: 1.4968571662902832\n",
      "Epoch: 267\n",
      "Loss: 1.4963622093200684\n",
      "Epoch: 268\n",
      "Loss: 1.4958680868148804\n",
      "Epoch: 269\n",
      "Loss: 1.4953750371932983\n",
      "Epoch: 270\n",
      "Loss: 1.4948828220367432\n",
      "Epoch: 271\n",
      "Loss: 1.4943915605545044\n",
      "Epoch: 272\n",
      "Loss: 1.493901014328003\n",
      "Epoch: 273\n",
      "Loss: 1.4934114217758179\n",
      "Epoch: 274\n",
      "Loss: 1.4929229021072388\n",
      "Epoch: 275\n",
      "Loss: 1.492435097694397\n",
      "Epoch: 276\n",
      "Loss: 1.4919483661651611\n",
      "Epoch: 277\n",
      "Loss: 1.4914624691009521\n",
      "Epoch: 278\n",
      "Loss: 1.49097740650177\n",
      "Epoch: 279\n",
      "Loss: 1.4904931783676147\n",
      "Epoch: 280\n",
      "Loss: 1.4900100231170654\n",
      "Epoch: 281\n",
      "Loss: 1.489527702331543\n",
      "Epoch: 282\n",
      "Loss: 1.4890460968017578\n",
      "Epoch: 283\n",
      "Loss: 1.488565444946289\n",
      "Epoch: 284\n",
      "Loss: 1.4880858659744263\n",
      "Epoch: 285\n",
      "Loss: 1.4876070022583008\n",
      "Epoch: 286\n",
      "Loss: 1.4871290922164917\n",
      "Epoch: 287\n",
      "Loss: 1.4866520166397095\n",
      "Epoch: 288\n",
      "Loss: 1.486175775527954\n",
      "Epoch: 289\n",
      "Loss: 1.4857003688812256\n",
      "Epoch: 290\n",
      "Loss: 1.485226035118103\n",
      "Epoch: 291\n",
      "Loss: 1.4847522974014282\n",
      "Epoch: 292\n",
      "Loss: 1.4842796325683594\n",
      "Epoch: 293\n",
      "Loss: 1.4838076829910278\n",
      "Epoch: 294\n",
      "Loss: 1.4833368062973022\n",
      "Epoch: 295\n",
      "Loss: 1.4828667640686035\n",
      "Epoch: 296\n",
      "Loss: 1.482397437095642\n",
      "Epoch: 297\n",
      "Loss: 1.4819289445877075\n",
      "Epoch: 298\n",
      "Loss: 1.4814612865447998\n",
      "Epoch: 299\n",
      "Loss: 1.4809948205947876\n",
      "Epoch: 300\n",
      "Loss: 1.4805289506912231\n",
      "Epoch: 301\n",
      "Loss: 1.480063796043396\n",
      "Epoch: 302\n",
      "Loss: 1.4795997142791748\n",
      "Epoch: 303\n",
      "Loss: 1.47913658618927\n",
      "Epoch: 304\n",
      "Loss: 1.4786739349365234\n",
      "Epoch: 305\n",
      "Loss: 1.4782123565673828\n",
      "Epoch: 306\n",
      "Loss: 1.4777517318725586\n",
      "Epoch: 307\n",
      "Loss: 1.4772918224334717\n",
      "Epoch: 308\n",
      "Loss: 1.476832628250122\n",
      "Epoch: 309\n",
      "Loss: 1.4763745069503784\n",
      "Epoch: 310\n",
      "Loss: 1.4759169816970825\n",
      "Epoch: 311\n",
      "Loss: 1.4754605293273926\n",
      "Epoch: 312\n",
      "Loss: 1.47500479221344\n",
      "Epoch: 313\n",
      "Loss: 1.4745497703552246\n",
      "Epoch: 314\n",
      "Loss: 1.4740957021713257\n",
      "Epoch: 315\n",
      "Loss: 1.4736427068710327\n",
      "Epoch: 316\n",
      "Loss: 1.473190188407898\n",
      "Epoch: 317\n",
      "Loss: 1.4727386236190796\n",
      "Epoch: 318\n",
      "Loss: 1.472287654876709\n"
     ]
    }
   ],
   "source": [
    "loss_fn =  Dice()\n",
    "loss= keras.losses.SparseCategoricalCrossentropy()\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.001, decay_steps=100, decay_rate=0.9, staircase=True)\n",
    "loss_value = 9.0    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# Iterate over the batches of a dataset.\n",
    "for epoch in range(1000):    \n",
    "    # Iterate over the batches of a dataset.        \n",
    "    for (i,m,l) in (train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = np.array(i)\n",
    "            #print(inputs.shape)\n",
    "            targets = np.array(m)\n",
    "            t = np.array(l)\n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            loss_value = tf.reduce_sum(loss_fn(targets,predictions[0])  + loss(t,predictions[1]))\n",
    "            tape.watch(loss_value)\n",
    "            gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "            #print(gradients)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "            print(\"Epoch:\", epoch)                 \n",
    "            print(\"Loss:\", float(loss_value))\n",
    "            if loss_value <= 0.23:\n",
    "                break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(PATH +'/classification/' + name)                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer=optimizer,  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=loss_value,\n",
    "    # List of metrics to monitor\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "epochs = 1000\n",
    "model.fit(inputsAll, (target,classLabel),\n",
    "          batch_size=5, epochs=epochs,)\n",
    "\n",
    "model.save(PATH +'/classification/' + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea102b4c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fea102b4c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n",
      "(40, 40, 40)\n",
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.measure import marching_cubes_lewiner\n",
    "import meshplot as mp\n",
    "layer_outputs = [layer.output for layer in model.layers] \n",
    "for i in np.array(inputsAll):   \n",
    "   # print(i.shape)\n",
    "    prediction = model.predict(np.array([i]))\n",
    "  #  print(np.round(prediction[1]))\n",
    "    pred_mask  = resize_data(prediction[0][:,:,:,0],i.shape) \n",
    "    pred=prediction[0]\n",
    "    print(pred[0][:,:,:,0].shape)\n",
    "    #vertices,faces,_,_ = marching_cubes_lewiner(pred[0][:,:,:,0])\n",
    "    print(np.round(prediction[1]))\n",
    "  #  mp.plot(vertices, faces, return_plot=False)\n",
    "\n",
    "  #  activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "  #  for l in range(len(layer_outputs)):\n",
    "  #      c =  activation_model.predict(np.array([i]))[l][0]\n",
    "  #      #print(np.array(c[0][:,:,0]).shape)\n",
    "  #      fig, (ax1) = plt.subplots(1,1)\n",
    " #       ax1.imshow(np.squeeze(c[0][:,:,0]), cmap=\"gray\")\n",
    "        \n",
    "#print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load('segmentation')\n",
    "train_classification(model,['mandible-Right-Molar/iteration1','mandible-Right-Molar/iteration2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load('segmentation')\n",
    "c_input = []\n",
    "labels =[]\n",
    "\n",
    "print(len(inputs_train)) \n",
    "training=[]\n",
    "for y in range((len(inputs_train))):\n",
    "    for i in (np.array(inputs_train)[y]): \n",
    "        mask = model.predict(np.array([np.expand_dims(i,3)])) \n",
    "        print(mask.shape)\n",
    "        mask = mask[0][:,:, :, 0]\n",
    "        mask =  np.round(resize(mask,(40,40,40)))\n",
    "        image = resize(i,(40,40,40))\n",
    "        print(mask.shape)\n",
    "      #  fig, (ax1) = plt.subplots(1,1)\n",
    "       # ax1.imshow(np.squeeze(image[20]), cmap=\"gray\")\n",
    "        \n",
    "        image[mask==0] = 0\n",
    "       # display_monatage(image,image)\n",
    "        c_input.append(image)\n",
    "        \n",
    "        #training.append(image)\n",
    "       # display_monatage(c_input[0],c_input[0])\n",
    "\n",
    "    print(np.array(target_train[y]).shape)\n",
    "    c = np.expand_dims(np.array(c_input),4)\n",
    "    \n",
    "    print(np.array(c).shape)\n",
    "    #print(training.shape)\n",
    "    target=np.concatenate(np.transpose(target_train),axis=-1)\n",
    "   # print(target)\n",
    "    training.append([c,target])\n",
    "    print(np.array(training).shape)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
